%% Template for ENG 401 reports
%% by Robin Turner
%% Adapted from the IEEE peer review template

%
% note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.

\documentclass[peerreview]{IEEEtran}
\usepackage{cite} % Tidies up citation numbers.
\usepackage{url} % Provides better formatting of URLs.
\usepackage[utf8]{inputenc} % Allows Turkish characters.
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables for horizontal lines
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stackengine}

\hyphenation{op-tical net-works semi-conduc-tor} % Corrects some bad hyphenation 



\begin{document}
%\begin{titlepage}
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Balloma Video Game playing Reinforcement Learning Agent.}


% author names and affiliations

\author{Luis Rojas Aguilera \\
Udacity\\
Capstone Project Proposal\\
}
\date{14/10/2019}

% make the title area
\maketitle
\tableofcontents
\listoffigures
\listoftables
%\end{titlepage}

\IEEEpeerreviewmaketitle
\begin{abstract}

  This work presents a capstone project proposal to achieve a Nanodegree in Machine Learning from Udacity. The objective of this work is to construct a video game playing robot, through the use of Deep Reinforcement Learning techniques. Also it should provide a methodology for test automation on game development process. This document provide descriptions on: \textit{a)} problem to be solved, \textit{b)} objectives, \textit{c)} context, \textit{d)} tools, \textit{e)} metrics and \textit{f)} techniques.

\end{abstract}


\section{Introduction}
Balloma is a single-player, android video game, developed by Black River Studios\footnote{http://blackriverstudios.net/} at brasilian SIDIA\footnote{https://www.sidia.com}.  During the game development process, after any new features are integrated, regression and functional tests must be executed in order to validate if new functionalities are properly working and side effects caused by new code have not appeared. 

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
\begin{figure}[!h]
\centering
\includegraphics[width=0.8\columnwidth]{img/balloma_cover.jpeg} 
\caption{Balloma splash screen.}
\label{fig_sim}
\end{figure}

Currently such tests are executed manually by humans, thus as functionalities' stack increases also does testing complexity and workload. In order to aim testing process this proposal presents a Proof Of Concept method for test automation using Deep Reinforcement Learning techniques. Next sections presents further details on Balloma video game, the RL framework and how to use it for game automatic controlling.

   
\section{Deep Reinforcement Learning}

 Reinforcement Learning (RL) is a field of Machine Learning that studies autonomous interactions among software agents and environments, and the way an agent can enhance its behavioral rules (policy) in order to maximize the reward it obtains from an environment. That is, at time step $t$, agent takes action $a_t$ on a given environment $E$ with transition dynamics $p(s_{t+1}|s_t, a_t)$ and current state $s_t$ obtaining a reward $r_t$ from a reward function $r(s_t, a_t)$ and transforming current state into $s_{t+1}$.
 
 Currently RL proposes two main approaches to represent an agent behavioral rules: a) Value-based and b) Policy-based algorithms. 
 
 In value-based algorithms it is used a function called action-value ($q_*(s,a)$) that contains the expected cumulative reward of performing a given action while in a given environment's state. Such function can be used by the agent to decide which action to take in a given state by selecting from $q$ the action that maximizes reward with a given probability $\epsilon$ that allows a desired exploration behavior so as it is applicable for non-deterministic environment dynamics. This approach is constrained to discrete action and state spaces.
 
 Policy-based algorithms directly maps states and actions by a approximated policy function that represents the underlying stochastic distribution presented by environment's dynamics. While in the Value-based approach an heuristic should be defined to provide exploration behavior and avoid conditional randomness influenced by agent greediness, in Policy-based such heuristic is expressed in the policy function and is not necessarily fixed for each possible action-space setup. This approach is applicable to continuous action and state spaces.
 
 For both approaches, agent behavior is guided by an optimal policy $\pi_*(s, a, \theta)$ evaluated by objective function $J(\theta)=\mathbb{E}[R(\tau)]$ of policy parameters $\theta$, based on state-action-reward expectations on trajectory $\tau=S_0, A_0, R_1, S_1, ... $ drawn from a probability distribution \textbf{$p(s',r|s, a)$}. In such case is convenient using the \textbf{Bellman Expectation Equation} \cite{bellman_equation} to represent expected return of taking an action $a_t$ while in state $s_t$ and following a policy $\pi$:  
 
 \begin{equation}
 	Q^\pi(s_t, a_t) = \mathbb{E}_{r_t, s_{t+1} \sim E} [r(s_t, a_t) + \gamma\mathbb{E}_{a_{t+1} \sim \pi} [Q^\pi(s_{t+1}, a_{t+1})]]
 \end{equation}
 
 In this proposal I intend to apply a hybrid approach for RL called Deep Deterministic Policy Gradients (DDPG) \cite{ddpg_2015}. It follows the actor-critic \cite{actor_critic_2000} algorithm DPG \cite{silver_2004} in which the agent's behavior is represented by function $\mu(s|\theta^\mu)$, called the actor, that is evaluated by the critic, a non-linear action-value function $Q(s,a)$ parametrized by $\theta^Q$, adjusted by minimizing the loss:
 
 \begin{equation}
   L(\theta^Q) = \mathbb{E}_{s_t \sim \rho^\beta, a_t \sim, r_t \sim E}[(Q(s_t,a_t|\theta^Q)- y_t)^2]
 \end{equation}
 
 where
 
 \begin{equation}
 	y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1})|\theta^Q)
 \end{equation}
 
Actor function $\mu(s|\theta^\mu)$ specifies the agent's policy by deterministically mapping states to a specific action. It is adjusted to maximize the expected reward from a start distribution $J=\mathbb{E}_{r_i, s_i \sim E, a_i \sim \pi}[R_1]$. That is attained by updating parameters $\theta^\mu$ following the policy's performance gradients, expressed as:

\begin{equation}
\begin{split}
	\nabla_{\theta^\mu} \approx \mathbb{E}_{s_t \sim \rho^\beta}[\nabla_{\theta^\mu}Q(s,a|\theta^Q)|_{s=s_t, a=\mu(s_t|\theta^\mu)}] \\	
	= \mathbb{E}_{s_t \sim \rho ^ \beta}[\nabla_a Q(s,a|\theta^Q)|_{s=s_t, a=\mu(s_t)} \nabla_{\theta_\mu} \mu(s|\theta^\mu)|_{s=s_t}]
	\end{split}
\end{equation}

 Both actor and critic functions are expressed by neuronal networks, updated following an approximation to a supervised learning approach. For that matter, target networks $\mu'(s|\theta^{u'})$ and $Q'(s,a|\theta^{Q'})$ are created, which are soft copies of actor and critic networks respectively. That soft copy is made from updated parameters as follows: $\theta'\leftarrow \tau \theta + (1-\tau)\theta'$ with $\tau \ll 1$
 
 In order to provide exploration capabilities, during training, a different policy than that of the agent is used, obtained by adding noise sampled from a noise process $\eta$ to the actor function. Such exploration policy is expressed as follows: 
 \begin{equation}
 	\mu'(s_t)=\mu(s_t|\theta_t^\mu ) + \eta
 \end{equation}
 
Training follows an episodic process similar to Q-Learning. In each episode a set of agent-environment interactions, represented as a transition tuple $(s_t, a_t, r_t, s_{t+1})$ are stored in a replay buffer $R$ \cite{replay_buffer_2015}. A set of tuples are sampled from $R$ and used to apply the updates previously explained in this section. This iterative process can be truncated based on optimization or time constraining conditions are met.


\section{Balloma video game}
  
   Balloma is a single-player video game that runs on Android devices. It is composed of several scenes that initially appears locked and gets unlocked as the user completes unlocked scene's objective. 
   
   Each scene contains a constrained little terrain world with elements arranged and boundaries that if crossed makes the scene end. The scene's main element is a ball the player should control and carry to a remarked target position in the scene. To control such ball the player should touch and swipe the device's screen inputing a direction and speed he wants the ball to take. Once the ball is positioned at target, scene ends, a score is given to user and next scene is unlocked. 
   
   \begin{figure}[!h]
		\centering
		\includegraphics[width=0.9\columnwidth]{img/balloma_scene.jpg} 
		\caption{Balloma's scene sample. Elements of scene are: a) The ball (blue), b) Floating elements (green), c) Score records (red), d) Target position (yellow)}
		\label{fig_scene}
	\end{figure}
   
   While the user carries the ball to target he can make the ball go trough floating elements in the scene which makes the score increase. Also a timer is included in the scene which influences the final score, lower scene's completion running times provides higher rewards.
   
   Figure \ref{fig_scene} presents the game's first scene which appears unlocked by default. In Figure \ref{fig_scene} the ball is bounded by a blue box. The blue diamonds bounded with a green box appears floating in the scene, if reached by the ball makes the score (upper left red box) increase. Target position is remarked inside a yellow box. Also there is a timer (lower left corner red box) that influences the final score.
   
   \section{Environment}
   
	An interface is implemented to provide a RL suitable environment by which the agent can interact with the game to construct the optimized policy. Since the game runs in Android, the environment should provide a controlling interface with an android physical or virtual device. Such functionality can be attained through the Android Debug Bridge (adb\footnote{https://developer.android.com/studio/command-line/adb}). 
	
	In this proposal the states are represented by raw scene frames of 240x240. It means at each time step $t$ the environment is at state $s_t \in \mathbb{R}^3$ formatted as a tensor of shape $(240, 240, 3)$. A similar approach was presented in \cite{ddpg_2015} and \cite{replay_buffer_2015}, however scenes in those works are 2D spaces and in this case it is 3D. Frames are cropped to exclude score indicators and focus on more important pixels.
	
	On each time step the agent can chose to take an action $a_t \in \mathbb{R}^4$. Actions are presented as a tuple $a_t=(x_0, x_1, y_0, y_1, \nu)$ containing swipe attributes: \textit{a)} $x_0$ starting swipe $x$ coordinate, \textit{b)} $x_1$ final swipe $x$ coordinate, \textit{c)} $y_0$ starting swipe $y$ coordinate, \textit{d)} $y_1$ final swipe $y$ coordinate and \textit{e)} swipe velocity $\nu$.
	
	Each episode starts with the first swipe inputted by the agent. If the balls falls or get to target position environment is set into a terminal state. Also it will be constrained in time to avoid long episodes. 
	
	\section{Reward function}
	
	At each time step, after the agent choses an action $a_t$ and such action is applied to environment, a reward $r_t$ is observed. Such reward is computed taking into account two coefficients: \textit{a)} the ratio of gathered floating diamonds over all such elements in the scene($\omega$) and \textit{b)} episode time passed since it started ($\varphi$). Then reward is calculated as:
	
	\begin{equation}
	  r(s_t, a_t) = \dfrac{\omega_t}{\varphi_t}
	\end{equation}
	
	  Such coefficients appears as elements of the scene at a fixed frame position. It should be cropped and processed so as to convert such pixels into a suitable representation of its digits. In order to extract the score on each time step from those scene elements, in this work it will be trained a digit recognizer on the well known MNIST handwritten digit database \cite{mnist}.	
	
 
   
\subsection{Your first solution}
Describe your first solution here.
\subsection{Your second solution}
Describe your second solution here.
\subsection{Your third solution}
Describe your third solution here.
\subsubsection{Subsubsection Heading Here}
Use the subsubsection command with caution---you probably won't need it at, but I'm including it this an example.

\section{Criteria for Assessing Solutions} \label{sec:criteria}
This may be a modified version of your proposal depending on previously carried out research or any feedback received.  



\section{Research Methodology}
The main difference between this section and the one in your report proposal is use of verb tense: there you suggested what you will do and here you will describe what you did. Be concise and precise when outlining how you researched your potential solutions. 
Remember that your research should be guided by: 
\begin{itemize}
\item Relevance to the context of application  
\item Your assessment criteria 
\item Practicality 
\end{itemize}
So it may be worth commenting on your research methodology in light of the above (e.g., justifying a particular approach).  

In this section, only describe how you collected data, and explain what you did to test your criteria.  \emph{Do not include your findings in this section.}

\section{Analysis and Interpretation}
 In this section you will mainly analyze your data in terms of your assessment criteria; e.g., do the data suggest that a particular solution is ``cost effective'' ``environmentally acceptable'', ``technically feasible'' or ``affordable''?
   
Be logical and selective when analyzing/interpreting your research data.  For example, if a proposed solution is proven to be far too expensive to realistically implement in your context, is there any value in discussing whether it is ``culturally viable'' or ``technically sustainable''? Perhaps in this case you can focus more attention on solutions that your research suggests are more valid.  Do not just throw huge quantities of raw data at your reader and leave them to interpret it. Present enough to transparently support any conclusions you draw and make sure that you offer justifications for your analysis.  

Be honest and reflective while discussing your data. Your data might be too limited or unclear to interpret with accuracy---explain this, perhaps suggesting how this shortcoming could be addressed. Admitting the above will help you draw more honest and worthwhile conclusions.  

Remember that research is an imperfect and ongoing process that should be open to question and verification. Therefore, unless convinced by the absolute strength of your evidence, you should be tentative in your language choice when interpreting/analyzing research results. Selectively use {\em hedging} (language which indicates a lack of certainty) to modify the tone of your analysis and any conclusions that result from this. 

Here are some examples that show differing degrees of certainty:
\begin{itemize}  
\item it appears that \ldots
\item it can be tentatively concluded that \ldots
\item it is almost certain that \ldots
\item perhaps the evidence indicates \ldots
\item this seems to point to the fact that \ldots
\item this could be interpreted as evidence of \ldots
\item without doubt its application would prove beneficial for \ldots
\end{itemize}

Finally, don’t introduce any new content (e.g., research methods or solutions) within this section---this will prove confusing for the reader. The reader should clearly understand that you are, based on specific criteria, interpreting the results of your research in order to test the viability of various solutions to remedy a particular problem. The sole function of this part of the report is to openly discuss your research findings in order to set up your conclusions/recommendations.


% Example of a table from http://www.latextemplates.com/template/professional-table
\begin{table} % Add the following just after the closing bracket on this line to specify a position for the table on the page: [h], [t], [b] or [p] - these mean: here, top, bottom and on a separate page, respectively
\centering % Centers the table on the page, comment out to left-justify
\begin{tabular}{l c c c c c} % The final bracket specifies the number of columns in the table along with left and right borders which are specified using vertical bars (|); each column can be left, right or center-justified using l, r or c. To specify a precise width, use p{width}, e.g. p{5cm}
\toprule % Top horizontal line
& \multicolumn{5}{c}{Growth Media} \\ % Amalgamating several columns into one cell is done using the \multicolumn command as seen on this line
\cmidrule(l){2-6} % Horizontal line spanning less than the full width of the table - you can add (r) or (l) just before the opening curly bracket to shorten the rule on the left or right side
Strain & 1 & 2 & 3 & 4 & 5\\ % Column names row
\midrule % In-table horizontal line
GDS1002 & 0.962 & 0.821 & 0.356 & 0.682 & 0.801\\ % Content row 1
NWN652 & 0.981 & 0.891 & 0.527 & 0.574 & 0.984\\ % Content row 2
PPD234 & 0.915 & 0.936 & 0.491 & 0.276 & 0.965\\ % Content row 3
JSB126 & 0.828 & 0.827 & 0.528 & 0.518 & 0.926\\ % Content row 4
JSB724 & 0.916 & 0.933 & 0.482 & 0.644 & 0.937\\ % Content row 5
\midrule % In-table horizontal line
\midrule % In-table horizontal line
Average Rate & 0.920 & 0.882 & 0.477 & 0.539 & 0.923\\ % Summary/total row
\bottomrule % Bottom horizontal line
\end{tabular}
\smallskip 
\caption{Some impressive numbers} % Table caption, can be commented out if no caption is required
\label{tab:template} % A label for referencing this table elsewhere, references are used in text as \ref{label}
\end{table}
A reference to Table \ref{tab:template}.

\section{Conclusions and Recommendations}
Conclusion shows what knowledge comes out of the report. As you draw a conclusion, you need to explain it in terms of the preceding discussion. You are expected to repeat the most important ideas you have presented, without copying. Adding a table/chart summarizing the results of your findings might be helpful for the reader to clearly see the most optimum solution(s). 

It is likely that you will briefly describe the comparative effectiveness and suitability of your proposed solutions. Your description will logically recycle language used in your assessing criteria (section \ref{sec:criteria}): ``Solution A proved to be the most cost effective of the alternatives'' or ``Solution B, though a viable option in other contexts, was shown to lack adaptability''.  Do not have detailed analysis or lengthy discussions in this section, as this should have been completed in section X. 
 
As for recommendations, you need to explain what actions the report calls for. These recommendations should be honest, logical and practical. You may suggest that one, a combination, all or none of your proposed solutions should be implemented in order to address your specific problem. You could also urge others to research the issue further, propose a plan of action or simply admit that the problem is either insoluble or has a low priority in its present state.   

The recommendations should be clearly connected to the results of the report, and they should be explicitly presented. Your audience should not have to guess at what you intend to say.  




\appendices
\section{What Goes in the Appendices} \label{App:WhatGoes}
The appendix is for material that readers only need to know if they are studying the report in depth. Relevant charts, big tables of data, large maps, graphs, etc. that were part of the research, but would distract the flow of the report should be given in the Appendices. 
\section{Formatting the Appendices} \label{App:Formatting}
Each appendix needs to be given a letter (A, B, C, etc.) and a title. \LaTeX will do the lettering automatically.


\begin{thebibliography}{1}
% Here are a few examples of different citations 
% Book

\bibitem{ddpg_2015}
  Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971 (2015).
  
  \bibitem {actor_critic_2000} 
  Konda, Vijay R., and John N. Tsitsiklis. "Actor-critic algorithms." Advances in neural information processing systems. 2000.
  
\bibitem{silver_2004}
  David~Silver, Guy~Lever, Nicolas~Heess, Thomas~Degris, Daan~Wierstra, and Martin~Riedmiller. 2014. Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32 (ICML'14), Eric P. Xing and Tony Jebara (Eds.), Vol. 32. JMLR.org I-387-I-395.
  
  \bibitem{replay_buffer_2015}
  
  Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,
Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Humanlevel control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

\bibitem{bellman_equation}

(2011) Bellman Equation. In: Sammut C., Webb G.I. (eds) Encyclopedia of Machine Learning. Springer, Boston, MA

\bibitem{mnist}

LeCun, Y. & Cortes, C. (2010), 'MNIST handwritten digit database', .
  
 
\end{thebibliography}

% This is a hand-made bibliography. If you want to use a BibTeX file, you're on your own ;-)














\end{document}


