\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Balloma splash screen.}}{1}}
\newlabel{fig_sim}{{1}{1}}
\citation{bellman_equation}
\citation{ddpg_2015}
\citation{actor_critic_2000}
\citation{silver_2004}
\@writefile{toc}{\contentsline {section}{\numberline {II}Deep Reinforcement Learning}{2}}
\citation{replay_buffer_2015}
\@writefile{toc}{\contentsline {section}{\numberline {III}Balloma video game}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Balloma's scene sample. Elements of scene are: a) The ball (blue), b) Floating elements (green), c) Score records (red), d) Target position (yellow)}}{3}}
\newlabel{fig_scene}{{2}{3}}
\citation{ddpg_2015}
\citation{replay_buffer_2015}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Environment}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}States}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Actions}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Reward function}{4}}
\citation{replay_buffer_2015}
\citation{ddpg_2015}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Benchmark Model}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Results}{5}}
\citation{ddpg_2015}
\citation{adam_2014}
\citation{relu_2011}
\citation{ddpg_2015}
\citation{adam_2014}
\citation{relu_2011}
\citation{rep_buffer}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-A}}Actor ConvNet}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Agent's Actor ConvNet Architecture.}}{6}}
\newlabel{tab:actor_arch}{{I}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-B}}Critic Neuronal Network}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VII-C}}Training}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Agent's Critic Neuronal Net Architecture.}}{7}}
\newlabel{tab:critic_arch}{{II}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cumulative Reward evolution through episodes.}}{7}}
\newlabel{fig:reward}{{3}{7}}
\citation{ddpg_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Action gradient loss evolution through subsequent steps.}}{8}}
\newlabel{fig:loss}{{4}{8}}
\bibcite{ddpg_2015}{1}
\bibcite{actor_critic_2000}{2}
\bibcite{silver_2004}{3}
\bibcite{replay_buffer_2015}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Action parameter's values evolution throughout training.}}{9}}
\newlabel{fig:loss}{{5}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Conclusions}{9}}
\@writefile{toc}{\contentsline {section}{References}{9}}
\bibcite{bellman_equation}{5}
\bibcite{mnist}{6}
\bibcite{adam_2014}{7}
\bibcite{relu_2011}{8}
\bibcite{rep_buffer}{9}
